* [[https://leetcode.com/problems/web-crawler-multithreaded][1242. Web
Crawler Multithreaded]]
  :PROPERTIES:
  :CUSTOM_ID: web-crawler-multithreaded
  :END:
[[./solution/1200-1299/1242.Web Crawler Multithreaded/README.org][中文文档]]

** Description
   :PROPERTIES:
   :CUSTOM_ID: description
   :END:

#+begin_html
  <p>
#+end_html

Given a url startUrl and an interface HtmlParser, implement a
Multi-threaded web crawler to crawl all links that are under the same
hostname as startUrl. 

#+begin_html
  </p>
#+end_html

#+begin_html
  <p>
#+end_html

Return all urls obtained by your web crawler in any order.

#+begin_html
  </p>
#+end_html

#+begin_html
  <p>
#+end_html

Your crawler should:

#+begin_html
  </p>
#+end_html

#+begin_html
  <ul>
#+end_html

#+begin_html
  <li>
#+end_html

Start from the page: startUrl

#+begin_html
  </li>
#+end_html

#+begin_html
  <li>
#+end_html

Call HtmlParser.getUrls(url) to get all urls from a webpage of given
url.

#+begin_html
  </li>
#+end_html

#+begin_html
  <li>
#+end_html

Do not crawl the same link twice.

#+begin_html
  </li>
#+end_html

#+begin_html
  <li>
#+end_html

Explore only the links that are under the same hostname as startUrl.

#+begin_html
  </li>
#+end_html

#+begin_html
  </ul>
#+end_html

#+begin_html
  <p>
#+end_html

#+begin_html
  </p>
#+end_html

#+begin_html
  <p>
#+end_html

As shown in the example url above, the hostname is example.org. For
simplicity sake, you may assume all urls use http protocol without
any port specified. For example, the urls http://leetcode.com/problems
and http://leetcode.com/contest are under the same hostname, while urls
http://example.org/test and http://example.com/abc are not under the
same hostname.

#+begin_html
  </p>
#+end_html

#+begin_html
  <p>
#+end_html

The HtmlParser interface is defined as such: 

#+begin_html
  </p>
#+end_html

#+begin_html
  <pre>
  interface HtmlParser {
    // Return a list of all urls from a webpage of given <em>url</em>.
    // This is a blocking call, that means it will do HTTP request and return when this request is finished.
    public List&lt;String&gt; getUrls(String url);
  }</pre>
#+end_html

#+begin_html
  <p>
#+end_html

Note that getUrls(String url) simulates performing a HTTP request. You
can treat it as a blocking function call which waits for a HTTP request
to finish. It is guaranteed that getUrls(String url) will return the
urls within 15ms.  Single-threaded solutions will exceed the time limit
so, can your multi-threaded web crawler do better?

#+begin_html
  </p>
#+end_html

#+begin_html
  <p>
#+end_html

Below are two examples explaining the functionality of the problem, for
custom testing purposes you'll have
three variables urls, edges and startUrl. Notice that you will only have
access to startUrl in your code, while urls and edges are not directly
accessible to you in code.

#+begin_html
  </p>
#+end_html

#+begin_html
  <p>
#+end_html

 

#+begin_html
  </p>
#+end_html

#+begin_html
  <p>
#+end_html

Follow up:

#+begin_html
  </p>
#+end_html

#+begin_html
  <ol>
#+end_html

#+begin_html
  <li>
#+end_html

Assume we have 10,000 nodes and 1 billion URLs to crawl. We will deploy
the same software onto each node. The software can know about all the
nodes. We have to minimize communication between machines and make sure
each node does equal amount of work. How would your web crawler design
change?

#+begin_html
  </li>
#+end_html

#+begin_html
  <li>
#+end_html

What if one node fails or does not work?

#+begin_html
  </li>
#+end_html

#+begin_html
  <li>
#+end_html

How do you know when the crawler is done?

#+begin_html
  </li>
#+end_html

#+begin_html
  </ol>
#+end_html

#+begin_html
  <p>
#+end_html

 

#+begin_html
  </p>
#+end_html

#+begin_html
  <p>
#+end_html

Example 1:

#+begin_html
  </p>
#+end_html

#+begin_html
  <p>
#+end_html

#+begin_html
  </p>
#+end_html

#+begin_html
  <pre>
  <strong>Input:
  </strong>urls = [
  &nbsp; &quot;http://news.yahoo.com&quot;,
  &nbsp; &quot;http://news.yahoo.com/news&quot;,
  &nbsp; &quot;http://news.yahoo.com/news/topics/&quot;,
  &nbsp; &quot;http://news.google.com&quot;,
  &nbsp; &quot;http://news.yahoo.com/us&quot;
  ]
  edges = [[2,0],[2,1],[3,2],[3,1],[0,4]]
  startUrl = &quot;http://news.yahoo.com/news/topics/&quot;
  <strong>Output:</strong> [
  &nbsp; &quot;http://news.yahoo.com&quot;,
  &nbsp; &quot;http://news.yahoo.com/news&quot;,
  &nbsp; &quot;http://news.yahoo.com/news/topics/&quot;,
  &nbsp; &quot;http://news.yahoo.com/us&quot;
  ]
  </pre>
#+end_html

#+begin_html
  <p>
#+end_html

Example 2:

#+begin_html
  </p>
#+end_html

#+begin_html
  <p>
#+end_html

#+begin_html
  </p>
#+end_html

#+begin_html
  <pre>
  <strong>Input:</strong> 
  urls = [
  &nbsp; &quot;http://news.yahoo.com&quot;,
  &nbsp; &quot;http://news.yahoo.com/news&quot;,
  &nbsp; &quot;http://news.yahoo.com/news/topics/&quot;,
  &nbsp; &quot;http://news.google.com&quot;
  ]
  edges = [[0,2],[2,1],[3,2],[3,1],[3,0]]
  startUrl = &quot;http://news.google.com&quot;
  <strong>Output:</strong> [&quot;http://news.google.com&quot;]
  <strong>Explanation: </strong>The startUrl links to all other pages that do not share the same hostname.</pre>
#+end_html

#+begin_html
  <p>
#+end_html

 

#+begin_html
  </p>
#+end_html

#+begin_html
  <p>
#+end_html

Constraints:

#+begin_html
  </p>
#+end_html

#+begin_html
  <ul>
#+end_html

#+begin_html
  <li>
#+end_html

1 <= urls.length <= 1000

#+begin_html
  </li>
#+end_html

#+begin_html
  <li>
#+end_html

1 <= urls[i].length <= 300

#+begin_html
  </li>
#+end_html

#+begin_html
  <li>
#+end_html

startUrl is one of the urls.

#+begin_html
  </li>
#+end_html

#+begin_html
  <li>
#+end_html

Hostname label must be from 1 to 63 characters long, including the dots,
may contain only the ASCII letters from 'a' to 'z', digits from '0' to
'9' and the hyphen-minus character ('-').

#+begin_html
  </li>
#+end_html

#+begin_html
  <li>
#+end_html

The hostname may not start or end with the hyphen-minus character
('-'). 

#+begin_html
  </li>
#+end_html

#+begin_html
  <li>
#+end_html

See:  https://en.wikipedia.org/wiki/Hostname#Restrictions_on_valid_hostnames

#+begin_html
  </li>
#+end_html

#+begin_html
  <li>
#+end_html

You may assume there're no duplicates in url library.

#+begin_html
  </li>
#+end_html

#+begin_html
  </ul>
#+end_html

** Solutions
   :PROPERTIES:
   :CUSTOM_ID: solutions
   :END:

#+begin_html
  <!-- tabs:start -->
#+end_html

*** *Python3*
    :PROPERTIES:
    :CUSTOM_ID: python3
    :END:
#+begin_src python
#+end_src

*** *Java*
    :PROPERTIES:
    :CUSTOM_ID: java
    :END:
#+begin_src java
#+end_src

*** *...*
    :PROPERTIES:
    :CUSTOM_ID: section
    :END:
#+begin_example
#+end_example

#+begin_html
  <!-- tabs:end -->
#+end_html
